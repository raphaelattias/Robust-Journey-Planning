{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import different required libraries, HIVE and Spark sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "                                                                                                \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "root_data = \"../data/\"\n",
    "username = 'parchet' # if you don't want to rederive all HDFS tables\n",
    "# username = os.environ['JUPYTERHUB_USER'] # If you whish to derive everything from scratch\n",
    "\n",
    "ipython.run_cell_magic('configure','-f','{{ \"name\":\"final-{0}\" }}'.format(username))\n",
    "print(\"Runnuing as user: \", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as SFunc\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "hiveaddr = os.environ['HIVE_SERVER_2']\n",
    "print(\"Operating as: {0}\".format(username))\n",
    "print(\"Operating on hiveaddr: {0}\".format(hiveaddr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from pyhive import hive\n",
    "\n",
    "# create connection\n",
    "conn = hive.connect(host=hiveaddr, \n",
    "                    port=10000,\n",
    "                    username=username) \n",
    "# create cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 150%\" class=\"alert alert-block alert-warning\">\n",
    "    Before jumping into the notebook, note that you can skip part 1 and 2 if you do not whish to rederive all the data from scratch! (you can set username='parchet' in cell 2 if you have never built the HDFS tables)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Data Import & Wrangling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. First of all, if you have never done so, prepare the required orc tables in your personnal HDFS by running the notebook `PrepareTablesHDFS.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Processing the geostops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the geostops both to pre-process the SBB istdaten and the timetables. As the geostops is quite small, we will simply process it once in a pandas dataframe for SBB istdaten usage and once in a spark Dataframe to use on the cluster.  \n",
    "Note that alternatively we could store it on HDFS to prevent processing the dataset twice but given it's size and low complexity we decided not to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import all the geostops from the previously created orc table in a local pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "query = \"\"\"\n",
    "    select STATIONID as id, REMARK as name, LATITUDE as lat, LONGITUDE as lon\n",
    "    from {0}.sbb_geostops\n",
    "\"\"\".format(username)\n",
    "geostops_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "geostops_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "data_types_dict = {'id': str, 'name': str, 'lat': float, 'lon': float}\n",
    "geostops_df = geostops_df.astype(data_types_dict)\n",
    "\n",
    "geostops_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will filter this dataframe to only keep the stops that are within the studied 15km around ZurichHB area: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deal here with a short distance (15km) and our accuracy doesn't have to be exact to the centimeter, so we can treat the surface of the earth as flat.\n",
    "So to perform our check we can just make a conversion from degrees to kilometers at the latitude of the center point, then Pythagore's theorem to get the distance.\n",
    "\n",
    "We could also use methods offered by libraries such as geopy / geo-py but this adds unnecessary complexity and additional library to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# Some constants to determine points within 15km from Zürich HB based on their (lat,lon) coordinates\n",
    "earth_radius = 6378.0\n",
    "zurich_avg_altitude = 0.430\n",
    "earth_circumference = 40075.0\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2, earth_circumference=earth_circumference):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two given points given their latitude and longitude coordinates\n",
    "    Code inspiration: https://stackoverflow.com/questions/24680247/check-if-a-latitude-and-longitude-is-within-a-circle-google-maps\n",
    "    \"\"\"\n",
    "    km_per_degree_lat = earth_circumference / 360.0\n",
    "    km_per_degree_lon = math.cos(math.pi * lat2 / 180.0) * km_per_degree_lat\n",
    "    dx = abs(lon2 - lon1) * km_per_degree_lon\n",
    "    dy = abs(lat2 - lat1) * km_per_degree_lat\n",
    "    return math.sqrt(dx*dx + dy*dy)\n",
    "\n",
    "def dist_from_center(lat_lon_row, central_lat=47.378177, central_lon=8.540192,earth_circumference=earth_circumference):\n",
    "    \"\"\"\n",
    "    Returns wether the distance of the given point (lat, long) from the central point (ZurichHB)\n",
    "    \"\"\"\n",
    "    return distance(lat_lon_row.lat, lat_lon_row.lon, central_lat, central_lon,earth_circumference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "max_dist=15.0\n",
    "geostops_df['center_dist'] = geostops_df.apply(dist_from_center, axis=1)\n",
    "zurich_geostops_df = geostops_df[geostops_df.center_dist <= max_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "zurich_geostops_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "zurich_geostops_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by considering only stops within Zürich area, we keep 1947 stops over the total 39026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "unique_stop_ids = len(set(zurich_geostops_df.id.tolist()))\n",
    "unique_stop_names = len(set(zurich_geostops_df.name.tolist()))\n",
    "\n",
    "print(\"Also remark that in those stops even so all %s stops have distinct Id, only %s have distinct names.\" %(unique_stop_ids, unique_stop_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# convert and save the dataframe to pickle\n",
    "pickle.dump(zurich_geostops_df, open(root_data+\"zurich_geostops_df.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similar procedure to create Spark Dataframe for timetable processing usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earth_circumference = 40075.0\n",
    "\n",
    "@SFunc.udf\n",
    "def distance(lat1, lon1, lat2=47.378177, lon2=8.540192, earth_circumference=earth_circumference):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two given points given their latitude and longitude coordinates\n",
    "    Code inspiration: https://stackoverflow.com/questions/24680247/check-if-a-latitude-and-longitude-is-within-a-circle-google-maps\n",
    "    \"\"\"\n",
    "    km_per_degree_lat = earth_circumference / 360.0\n",
    "    km_per_degree_lon = math.cos(math.pi * lat2 / 180.0) * km_per_degree_lat\n",
    "    dx = abs(lon2 - lon1) * km_per_degree_lon\n",
    "    dy = abs(lat2 - lat1) * km_per_degree_lat\n",
    "    return math.sqrt(dx*dx + dy*dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist=15.0\n",
    "\n",
    "geostops = spark.read.orc(\"/data/sbb/orc/geostops\")\n",
    "geostops = geostops.withColumn('distance', distance(geostops['stop_lat'], geostops['stop_lon'])).filter(SFunc.col('distance') <= max_dist)\n",
    "geostops = geostops.drop('location_type', 'parent_station')\n",
    "geostops.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Processing the timetables data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dataframe corresponding only to the schedules on May 13-17, 2019. As this is a typical week schedule, we will use it as our base timetable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import calendar data\n",
    "\n",
    "calendar = spark.read.csv(\"/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv\", header=True, encoding='utf8')\n",
    "calendar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import trips data\n",
    "\n",
    "trips = spark.read.csv(\"/data/sbb/csv/timetable/trips/2019/05/07/trips.csv\", header=True, encoding='utf8')\n",
    "trips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import routes data\n",
    "\n",
    "routes = spark.read.csv(\"/data/sbb/csv/timetable/routes/2019/05/07/routes.csv\", header=True, encoding='utf8')\n",
    "routes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stop_time data\n",
    "\n",
    "timetable = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True, encoding='utf8')\n",
    "timetable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist of all we noted that most of those trips have duration under 1min (which makes sense for all bus stops close in location):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_count = timetable.count()\n",
    "long_trips_count = timetable[timetable.departure_time != timetable.arrival_time].count()\n",
    "print(\"Over the {0} stops, only {1} have duration higher than a minute.\".format(trips_count, long_trips_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's now only keep the trips that were made within our area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all stop_id that are in Zurich\n",
    "zurich_stops = set([str(stop.stop_id) for stop in geostops.select('stop_id').collect()])\n",
    "\n",
    "#filter the timetable to only contains Stops that are in Zurich\n",
    "zurich_timetable=timetable.filter(F.col('stop_id').isin(zurich_stops))\n",
    "zurich_timetable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We generate the mapping between trip_id and the transport type\n",
    "- First, generate all pairs trip_id & transport_type that are in Zürich\n",
    "- Second, display every distinct transport type\n",
    "- Third, create a dictionnary that will map the precise transport type label into 4 simple type $\\in$ {Train,Tram,Bus,NotSupported}\n",
    "- Fourth, replace all the precise type by the simple ones\n",
    "- Fifth, send it to local to convert it into a dictionnary and save it as a pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o tripID_to_ttype -n -1\n",
    "\n",
    "#First\n",
    "print(\"First\")\n",
    "tripID_to_ttype=timetable.select(\"trip_id\",\"stop_id\")\\\n",
    "                        .filter(F.col('stop_id').isin(zurich_stops))\\\n",
    "                        .drop(\"stop_id\").distinct()\\\n",
    "                        .join(trips.select(\"route_id\",\"trip_id\"),\"trip_id\")\\\n",
    "                        .join(routes.select(\"route_id\",\"route_desc\"),\"route_id\").drop(\"route_id\")\\\n",
    "                        .withColumnRenamed(\"route_desc\",\"ttype\")\n",
    "\n",
    "tripID_to_ttype.show(3)\n",
    "\n",
    "#Second\n",
    "print(\"Second\\nAll distinct transport types\")\n",
    "tripID_to_ttype.select(\"ttype\").distinct().show()\n",
    "\n",
    "#Third\n",
    "# value in {Train,Tram,Bus,NotSupported}\n",
    "precise_to_simple_ttype= {\n",
    "    \"TGV\":\"Train\",\n",
    "    \"Eurocity\":\"Train\",\n",
    "    \"Regionalzug\":\"Train\",\n",
    "    \"RegioExpress\":\"Train\",\n",
    "    \"S-Bahn\":\"Train\",\n",
    "    \"Tram\":\"Tram\",\n",
    "    \"ICE\":\"Train\",\n",
    "    \"Bus\":\"Bus\",\n",
    "    \"Eurostar\":\"Train\",\n",
    "    \"Intercity\":\"Train\",\n",
    "    \"InterRegio\":\"Train\",\n",
    "    \"Extrazug\":\"Train\"\n",
    "}\n",
    "\n",
    "@SFunc.udf\n",
    "def precise_to_simple_ttype_function(key):\n",
    "    return precise_to_simple_ttype.get(key,\"NotSupported\")\n",
    "\n",
    "#Fourth\n",
    "print(\"Fourth\")\n",
    "tripID_to_ttype=tripID_to_ttype.withColumn(\"ttype\",precise_to_simple_ttype_function(tripID_to_ttype[\"ttype\"]))\n",
    "tripID_to_ttype.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "#Fifth\n",
    "tripID_to_ttype_dict=tripID_to_ttype.set_index(\"trip_id\")[\"ttype\"].T.to_dict()\n",
    "\n",
    "pickle.dump(tripID_to_ttype_dict, open(root_data+\"tripID_to_ttype_dict.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create all the connections in our dataframe and the footpath connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#partition by trip_id order by arrival_time and get the next stop_id and next arrival_time\n",
    "#and join with calendar and trips dataframe in order to retrieve the weekday\n",
    "\n",
    "rolling_pair_window=Window.partitionBy(\"trip_id\").orderBy(\"arrival_time\")\n",
    "\n",
    "next_arrival=F.lead(\"arrival_time\").over(rolling_pair_window).alias(\"arr_time\")\n",
    "next_stop_id=F.lead(\"stop_id\").over(rolling_pair_window).alias(\"arr_stop\")\n",
    "\n",
    "connections=zurich_timetable.select(\"trip_id\",\"departure_time\",\"stop_id\",next_arrival,next_stop_id)\\\n",
    "                            .na.drop(\"any\")\\\n",
    "                            .withColumnRenamed(\"stop_id\",\"dep_stop\")\\\n",
    "                            .withColumnRenamed(\"departure_time\",\"dep_time\")\\\n",
    "                            .where(\"SUBSTR(dep_time,1,2)>=7 and SUBSTR(arr_time,1,2)<=20\")\\\n",
    "                            .join(trips.select(\"trip_id\",\"service_id\"),\"trip_id\")\\\n",
    "                            .join(calendar,\"service_id\")\\\n",
    "                            .drop(\"service_id\",\"start_date\",\"end_date\")\n",
    "\n",
    "connections.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_foot_distance=0.5 # in km\n",
    "walking_speed_kmPerMin=0.05 # in km/min\n",
    "\n",
    "# determine the duration (in minutes)\n",
    "footpaths=geostops.alias('l').join(geostops.alias('r'))\\\n",
    "                .where('abs(r.distance- l.distance)<{0} and l.stop_id<>r.stop_id'.format(max_foot_distance))\\\n",
    "                .select(F.col('l.stop_id').alias('dep_stop'),\n",
    "                        F.col('r.stop_id').alias('arr_stop'),\n",
    "                        distance(F.col('l.stop_lat'),F.col('l.stop_lon'),F.col('r.stop_lat'),F.col('r.stop_lon')).alias('distance'))\\\n",
    "                .where('distance<{0}'.format(max_foot_distance))\\\n",
    "                .select('dep_stop',\n",
    "                        'arr_stop',\n",
    "                        (F.col('distance')/walking_speed_kmPerMin).alias('dur'))\n",
    "\n",
    "footpaths.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Processing the required istDaten SBB data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we will first make an external table only containing all the journeys that:\n",
    "- Are between two stations within 15km of Zurich main train station ('Zürich HB (8503000)', lat=47.378177, lon=8.540192)\n",
    "- Standard date of trip format\n",
    "- Non empty product id\n",
    "- for Trains and Bus we check AN_PROGNOSE_STATUS and AB_PROGNOSE_STATUS are equal to REAL or GESCHAETZT. \n",
    "- After some more research it appears Trams in Zurich do not have REAL or GESCHAETZT prognose so for this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we realized that on the Swiss scale, all the main means of transport have a fair share of journeys with prognostics either REAL or GESCHAETZT but in Zurich in particular, there are not enough tram journeys with these information to provide us an estimate of the delays.\n",
    "\n",
    "Therefore, for this particular mean of transport, we will also allow forcasted values (PROGNOSE) so that we can determine an approximate estimate of the trams delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "within_15_stop_stations = tuple(set(zurich_geostops_df.name.tolist()))\n",
    "\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.zurich_istdaten\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query = \"\"\"\n",
    "    create external table {0}.zurich_istdaten\n",
    "    as\n",
    "    select FAHRT_BEZEICHNER as trip_id, lower(PRODUKT_ID) as ttype, LINIEN_ID as train_nb, FAELLT_AUS_TF as trip_failed, DURCHFAHRT_TF as no_stop,\n",
    "    HALTESTELLEN_NAME as stop_name, ZUSATZFAHRT_TF as unplanned_trip, LINIEN_TEXT as linien, VERKEHRSMITTEL_TEXT as verkehrsmittel,\n",
    "    unix_timestamp(ANKUNFTSZEIT, 'dd.MM.yyyy HH:mm') as expected_ar, unix_timestamp(AN_PROGNOSE,'dd.MM.yyyy hh:mm:ss') as actual_ar,\n",
    "    unix_timestamp(ABFAHRTSZEIT, 'dd.MM.yyyy HH:mm') as expected_dep, unix_timestamp(AB_PROGNOSE,'dd.MM.yyyy hh:mm:ss') as actual_dep\n",
    "    from {0}.sbb_orc\n",
    "    where BETRIEBSTAG like '__.__.____' and PRODUKT_ID is not NULL and PRODUKT_ID <> ''\n",
    "    and HALTESTELLEN_NAME in {1}\n",
    "    and ( (AN_PROGNOSE_STATUS in ('REAL', 'GESCHAETZT') and AB_PROGNOSE_STATUS in ('REAL', 'GESCHAETZT')) \n",
    "    or (lower(PRODUKT_ID)=='tram' and AN_PROGNOSE_STATUS!='UNBEKANNT' and AB_PROGNOSE_STATUS!='UNBEKANNT') ) \n",
    "\"\"\".format(username, within_15_stop_stations)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "query = \"\"\"\n",
    "    select *, floor((actual_ar-expected_ar)/(12))\n",
    "    from {0}.zurich_istdaten\n",
    "    where ttype = 'bus' and extract(hour from FROM_UNIXTIME(expected_ar)) = 12 and ((floor(expected_ar/86400) + 4) % 7+1) = 1\n",
    "    limit 5\n",
    "\"\"\".format(username)\n",
    "tr_sbb_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(tr_sbb_df.columns)\n",
    "tr_sbb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Determine delays heuristically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We query from the previously created external HDFS table zurich_istdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Set at 12, every delay (in seconds) will be grouped by bucket of size 12 seconds : [0,11], [12,25], .. \n",
    "bucket_size = 12 \n",
    "query=\"\"\"\n",
    "    (select S.ttype as ttype,  S.day as day, S.hour as hour, S.delay as delay, count(*) as count \n",
    "    FROM (SELECT t.ttype, (floor(t.expected_ar/86400) + 4) % 7 as day, extract(hour from FROM_UNIXTIME(t.expected_ar)) as hour, floor((t.actual_ar-t.expected_ar)/{1})*{1}/60 as delay\n",
    "    FROM {0}.zurich_istdaten T) S\n",
    "    WHERE s.delay <= 8 and S.delay >= 0\n",
    "    GROUP BY S.ttype,  S.day, S.delay, S.hour\n",
    "    ORDER BY S.ttype,  S.day, S.delay, S.hour)\n",
    "\"\"\".format(username,bucket_size)\n",
    "dis = pd.read_sql(query, conn)\n",
    "\n",
    "ax = dis[(dis['ttype'] == 'zug') & (dis['day'] == 0) & (dis['hour'] == 8)].plot.bar(x='delay',y='count')\n",
    "ax.set_ylabel('Number of delays')\n",
    "ax.set_xlabel('Delay in minutes')\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Create dictionnary of delays distributions for Bus, Trams and Trains depending on the day and hour (working hours)\n",
    "cdf = {} \n",
    "\n",
    "for ttype in ['bus','zug','tram']:\n",
    "    cdf_day = {}\n",
    "    for day in [0,1,2,3,4,5,6]:\n",
    "        cdf_hour = {}\n",
    "        for hour in range(6,22):\n",
    "            hist = dis[(dis['ttype'] == ttype) & (dis['day'] == day) & (dis['hour'] == hour)]['count'].to_numpy()\n",
    "            while len(hist) < 41 and len(hist) != 0 :\n",
    "                hist = np.append(hist,0.0)\n",
    "            if len(hist) == 0:\n",
    "                hist = np.zeros(41)\n",
    "                hist[0] = 1\n",
    "                \n",
    "            cdf_hour[hour] = np.cumsum(hist/hist.sum())\n",
    "        cdf_day[day] = cdf_hour\n",
    "    cdf[ttype]=cdf_day\n",
    "    \n",
    "cdf['Bus'] = cdf['bus']\n",
    "cdf['Train'] = cdf['zug']\n",
    "cdf['Tram'] = cdf['tram']\n",
    "\n",
    "del cdf['bus']\n",
    "del cdf['zug']\n",
    "del cdf['tram']\n",
    "\n",
    "#save it as a pickle object\n",
    "pickle.dump(cdf, open(root_data+\"cdf.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "# Cumulative Distribution Function of the delay (not arrival time, the *delay*) of a BUS on WEDNESDAY at 10:00\n",
    "plt.ylabel('P(delay < x)')\n",
    "plt.xlabel(\"Delay in Minutes\")\n",
    "plt.plot(np.linspace(0,8,41),cdf['Bus'][0][9]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Find the best journeys: the Connection Scan Algorithm (CSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save in Local the required datasets: Connections and Footpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "#list the files\n",
    "!git lfs ls-files \n",
    "# Pull from git lfs our pickles\n",
    "!git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from collections import defaultdict\n",
    "\n",
    "#define function to be wrapped around the import of cdf\n",
    "def from_cdf_to_default(cdf):\n",
    "    cdf_day = {}\n",
    "    for day in [0,1,2,3,4,5,6]:\n",
    "        cdf_hour = {}\n",
    "        for hour in range(6,22):\n",
    "            current_cdf = np.zeros(41)\n",
    "            for ttype in ['Bus','Train','Tram']:\n",
    "                current_cdf += cdf[ttype][day][hour]\n",
    "            cdf_hour[hour] = current_cdf/3\n",
    "        cdf_day[day] = cdf_hour\n",
    "        \n",
    "    return defaultdict(lambda:cdf_day,cdf)\n",
    "\n",
    "# load dataset\n",
    "\n",
    "# Geostops\n",
    "geostops=pickle.load(open(root_data+\"zurich_geostops_df.pickle\",'rb'))\n",
    "\n",
    "# Footpaths\n",
    "footpaths = np.array(pd.read_pickle(root_data+\"footpaths.pickle\"))\n",
    "\n",
    "# Connections\n",
    "connections = pd.read_pickle(root_data+\"connections.pickle\")\n",
    "\n",
    "trips = connections['trip_id'].unique()\n",
    "\n",
    "connections['dep_time']=pd.to_datetime(connections['dep_time'])\n",
    "connections['arr_time']=pd.to_datetime(connections['arr_time'])\n",
    "\n",
    "# split the connections by week day\n",
    "weekday_list=[\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"]\n",
    "connections_week=[]\n",
    "for idx,weekday in enumerate(weekday_list):\n",
    "    temp = connections[connections[weekday]=='1'][['trip_id','dep_time','dep_stop','arr_time','arr_stop']].sort_values(by=['arr_time'])\n",
    "    connections_week.append( np.array(temp) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSA algorithm setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "stops = geostops['id']\n",
    "stops_names = dict(zip(stops,geostops['name']))\n",
    "tripID_to_ttype_dict=pickle.load(open(root_data+\"tripID_to_ttype_dict.pickle\",'rb'))\n",
    "cdf=from_cdf_to_default(pickle.load(open(root_data+\"cdf.pickle\",'rb')))\n",
    "\n",
    "date_init = pd.to_datetime('00:00:00')\n",
    "delta = pd.to_timedelta(2, unit='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Descriprtion \n",
    "##### (revisited CSA algorithm to maximize the departure time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![algo](../figs/algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "def CSA(starting_stop, arrival_stop, arrival_time, weekday, avoid=[]):\n",
    "    '''\n",
    "    Connections Scan Algorithm to maximize the starting time\n",
    "    \n",
    "    Return: List of trips, where each trip is a list of 5 elements: \n",
    "                [means of transport (trip ID or 'walking'), starting time (datetime), starting_stop (name), arriving_time (datetime), arriving stop (name)]\n",
    "                \n",
    "    '''   \n",
    "    \n",
    "    S = dict.fromkeys(stops, [None, date_init, None, None, None]) # {from: [starting_time, to, how]}\n",
    "    T = dict.fromkeys(trips, False)\n",
    "    S[arrival_stop] = [None, arrival_time, arrival_stop, None, None]\n",
    "    for footpath in footpaths[ footpaths[:,0]==arrival_stop ]:\n",
    "        S[footpath[1]] = ['Walking', arrival_time-footpath[2], footpath[1], arrival_time, arrival_stop]\n",
    "    \n",
    "    \n",
    "    connections = connections_week[weekday]\n",
    "    lowest = 0\n",
    "    highest = connections.shape[0]\n",
    "    \n",
    "    def binary_search(tau, low=lowest, high=highest):\n",
    "        '''\n",
    "        Binary Search \n",
    "        \n",
    "        Return: An integer, Index of the first connection arring at 'tau' \n",
    "        '''\n",
    "        if high > low:\n",
    "            mid = (high + low) // 2\n",
    "            arr_time = connections[mid,3]\n",
    "\n",
    "            if arr_time == tau:\n",
    "                return mid\n",
    "            elif arr_time > tau:\n",
    "                return binary_search(tau, low, mid - 1)\n",
    "            else:\n",
    "                return binary_search(tau, mid + 1, high)\n",
    "\n",
    "        if high == low:\n",
    "            mid = high\n",
    "            arr_time = connections[mid,3]\n",
    "            if arr_time <= tau:\n",
    "                return mid\n",
    "            else:\n",
    "                return mid-1\n",
    "\n",
    "        else:\n",
    "            return TypeError('Wrong Inizialiaztion in Binary Search: \\'high\\' has to be at least as big as \\'low\\'')\n",
    "    \n",
    "    \n",
    "    c_0 = binary_search(arrival_time)\n",
    "    for connection in connections[c_0::-1]:\n",
    "        if not (connection[0] in avoid): \n",
    "            if S[starting_stop][1] >= connection[3]:\n",
    "                break\n",
    "            if T[connection[0]] or S[connection[4]][1]>=connection[3]+delta:\n",
    "                if S[connection[2]][1]<connection[1]:\n",
    "                    T[connection[0]] = True\n",
    "                    S[connection[2]] = list(connection)\n",
    "                    for footpath in footpaths[ footpaths[:,0]==connection[2] ]:\n",
    "                        if S[footpath[1]][1]<(connection[1]-(footpath[2])):\n",
    "                            S[footpath[1]] = ['Walking', connection[1]-footpath[2], footpath[1], connection[1], footpath[0]]                        \n",
    "\n",
    "    if S[starting_stop][0]!=None:\n",
    "        path = []\n",
    "        mof = None\n",
    "        step = S[starting_stop]\n",
    "        while step[0]!=None:\n",
    "            if step[0]!=mof:\n",
    "                if step[2]!=starting_stop:\n",
    "                    path[-1][3] = old[3]\n",
    "                    path[-1][4] = old[4]\n",
    "                path.append(step)\n",
    "                mof = step[0]\n",
    "                old = step\n",
    "            else:\n",
    "                old = step\n",
    "            step = S[step[4]]\n",
    "        if step[0]!=mof:\n",
    "            path[-1][3] = old[3]\n",
    "            path[-1][4] = old[4]\n",
    "        path.append(step)\n",
    "        return path\n",
    "    \n",
    "    else:\n",
    "        #print('No path exists starting from \"{}\" and arriving to \"{}\" by {} on {}'.format(starting_stop, arrival_stop, arrival_time, weekday_list[weekday]))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "def describe(path):\n",
    "    prob = confidence_interval(path)[0]\n",
    "    print('Confidence Level: {0:.2f}%, Departure Time: {1}\\n'.format(prob*100, path[0][1].time()))\n",
    "    for step in path:\n",
    "        if step[0]!=None:\n",
    "            if step[0]!='Walking':\n",
    "                print('From \"{}\" at {} take {}-\"{}\" reaching \"{}\" at {}'.format(stops_names[step[2]], step[1].time(),tripID_to_ttype_dict[step[0]],step[0], stops_names[step[4]], step[3].time()))\n",
    "            else: \n",
    "                print('From \"{}\" walk up to \"{}\" ({} minutes needed)'.format(stops_names[step[2]], stops_names[step[4]], int((step[3]-step[1]).total_seconds()/60) ))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "def confidence_interval(path, verbose=False):\n",
    "    p = 1\n",
    "    min_p = 1\n",
    "    min_trip = 0\n",
    "\n",
    "    for i,trip in enumerate(path): \n",
    "\n",
    "        walking_between_stops = (trip[0] == 'Walking'  and i != len(path)-1 and i != 0)\n",
    "        walking_in_a_stop = (i != 0 and trip[0] != 'Walking' and path[i-1][0] != 'Walking'  and trip[0] != path[i-1][0])\n",
    "        current_p = 1\n",
    "        if walking_between_stops or walking_in_a_stop:\n",
    "            day = path[i-1][3].weekday()\n",
    "            hour = path[i-1][3].hour\n",
    "            ttype = tripID_to_ttype_dict[path[i-1][0]]\n",
    "            current_p = 1\n",
    "            if walking_between_stops:\n",
    "                time_to_change = (path[i+1][1]-path[i-1][3]).seconds\n",
    "                time_to_walk = (trip[3]-trip[1]).seconds\n",
    "            elif walking_in_a_stop:\n",
    "                time_to_change = (path[i][1]-path[i-1][3]).seconds\n",
    "                time_to_walk = (120)\n",
    "            if (time_to_change-time_to_walk)/60 < 8:\n",
    "                current_p = cdf[ttype][day][hour][int((time_to_change-time_to_walk)/12)] # Probability that the (delay + time_to_walk) <= transition time \n",
    "                #print(time_to_change/60,time_to_walk/60,current_p)\n",
    "                if current_p < min_p:\n",
    "                    min_p = current_p\n",
    "                    min_trip = path[i-1][0]\n",
    "\n",
    "        p *= current_p\n",
    "        \n",
    "    if verbose: print(\"The probability of arriving in time is {}, and the trip the least likely to be made in time is {} with probability {}\".format(p, min_trip, min_p))\n",
    "        \n",
    "    return p, min_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "def query(from_, to_, by_, weekday, min_prob, avoid=[]):\n",
    "    \n",
    "    alternatives = []\n",
    "    brute_force = (min_prob<0.99)\n",
    "    i = 0\n",
    "    \n",
    "    if from_ == to_:\n",
    "        raise ValueError('Starting stop and arrival stop are equal! Please choose two different stops.')\n",
    "    \n",
    "    path = CSA(from_, to_, by_, weekday, avoid)\n",
    "    prob, dang = confidence_interval(path)\n",
    "    if prob>min_prob:\n",
    "            alternatives.append(path)\n",
    "            min_prob=prob\n",
    "    avoid.append(dang)\n",
    "    \n",
    "    while min_prob*(brute_force)<0.99 and i<50:\n",
    "        \n",
    "        path = CSA(from_, to_, by_, weekday, avoid)\n",
    "        prob, dang = confidence_interval(path)\n",
    "        \n",
    "        if prob==1:\n",
    "            alternatives.append(path)\n",
    "            break\n",
    "        \n",
    "        if prob>min_prob:\n",
    "            alternatives.append(path)\n",
    "            min_prob=prob\n",
    "        \n",
    "        avoid.append(dang)\n",
    "        i +=1\n",
    "        \n",
    "        \n",
    "    if alternatives==[]: \n",
    "        print('No path exists for such a query')\n",
    "    \n",
    "    return alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that all our required functions are defined, we can try out a first manual journey search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "from_ = '8591122'\n",
    "to_ = '8503500'\n",
    "by_ = pd.to_datetime('18:00:00')\n",
    "weekday = 1 # Tuesday\n",
    "Q = 0.95\n",
    "avoid = []\n",
    "\n",
    "print('From: ', stops_names[from_])\n",
    "print('To: ', stops_names[to_])\n",
    "print('By: ', by_.time())\n",
    "print('On: ', weekday_list[weekday].capitalize())\n",
    "print('Minimum Confidence Level: {}%\\n\\n'.format(Q*100))\n",
    "\n",
    "\n",
    "paths = query(from_, to_, by_, weekday, Q, avoid)\n",
    "n_paths = len(paths)\n",
    "if n_paths==1: print('An optimal path is found!')\n",
    "else: print('{} optimal paths are found!'.format(n_paths))\n",
    "        \n",
    "for i,path in enumerate(paths):\n",
    "    print('\\n\\nAlternative: ', i+1)\n",
    "    describe(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate also the average computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import random\n",
    "import time\n",
    "\n",
    "n = 100\n",
    "A = random.sample(list(stops), n)\n",
    "B = random.sample(list(stops), n)\n",
    "by_ = pd.to_datetime('13:00:00')\n",
    "weekday = 1 # Tuesday\n",
    "\n",
    "times = []\n",
    "for a,b in zip(A,B):\n",
    "    start = time.time()\n",
    "    CSA(a, b, by_, weekday)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "\n",
    "times = np.array(times)\n",
    "mean = np.mean(times)\n",
    "std = np.std(times)\n",
    "\n",
    "print('The average computation time is {0:.2f}±{1:.2f} seconds'.format(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Vizualization and interactive interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "def visualize_path(path):\n",
    "    #re-import geostops to improve independence\n",
    "    geostops=pickle.load(open(root_data+\"zurich_geostops_df.pickle\",'rb'))\n",
    "    #to retrieve the coordinate easier\n",
    "    geostops_id=geostops.set_index(\"id\")\n",
    "    \n",
    "    #dict to map transport type to color and to accumulator\n",
    "    ttype_to_color={\"Walking\":\"#4A48D9\",\"Train\":\"#FFAE00\",\"Tram\":\"#8000FF\",\"Bus\":\"#33A140\",\"NotSupported\":\"#FF0000\"}\n",
    "    already_created_legend={\"Walking\":0,\"Train\":0,\"Tram\":0,\"Bus\":0,\"NotSupported\":0}\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    #for each section in the path, display it\n",
    "    for trip_ID,_,stop0,_,stop1 in path[:-1]:\n",
    "        lat_0,lon_0=geostops_id.loc[stop0][[\"lat\",\"lon\"]]\n",
    "        lat_1,lon_1=geostops_id.loc[stop1][[\"lat\",\"lon\"]]\n",
    "        ttype=tripID_to_ttype_dict.get(trip_ID,\"Walking\")\n",
    "\n",
    "        display=False\n",
    "        if already_created_legend[ttype]==0:\n",
    "            display=True\n",
    "            already_created_legend[ttype]=1\n",
    "\n",
    "        fig.add_trace(go.Scattermapbox(mode = \"lines\",\n",
    "                                        lon = [lon_0,lon_1],\n",
    "                                        lat = [lat_0,lat_1],\n",
    "                                        name=ttype,\n",
    "                                        showlegend=display,\n",
    "                                        marker = {'color':ttype_to_color[ttype]},\n",
    "                                        line={'width':4}))\n",
    "\n",
    "        fig.add_trace(go.Scattermapbox(mode = \"markers\",\n",
    "                                        lon = [lon_0],\n",
    "                                        lat = [lat_0],\n",
    "                                        hovertext=geostops_id.loc[stop0][\"name\"],\n",
    "                                        showlegend=False,\n",
    "                                        marker = {'size': 20,'color':ttype_to_color[ttype]}))\n",
    "\n",
    "    #for the last point\n",
    "    trip_ID=path[-2][0]\n",
    "    stop0=path[-1][2]\n",
    "    lat_0,lon_0=geostops_id.loc[stop0][[\"lat\",\"lon\"]]\n",
    "    ttype=tripID_to_ttype_dict.get(trip_ID,\"Walking\")\n",
    "    fig.add_trace(go.Scattermapbox(mode = \"markers\",\n",
    "                                    lon = [lon_0],\n",
    "                                    lat = [lat_0],\n",
    "                                    hovertext=geostops_id.loc[stop0][\"name\"],\n",
    "                                    showlegend=False,\n",
    "                                    marker = {'size': 20,'color':ttype_to_color[ttype]}))\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin ={'l':0,'t':0,'b':0,'r':0},\n",
    "        mapbox = {\n",
    "            'center': {'lon': 8.540192, 'lat':47.378177 },\n",
    "            'style': \"open-street-map\",\n",
    "            'zoom':11})\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive journey planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "all_stop_n_id=list(map(lambda x:tuple(x),geostops[[\"name\",\"id\"]].sort_values(by=\"name\").values))\n",
    "\n",
    "slider=widgets.FloatSlider(\n",
    "    value=0.95,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "start=widgets.Dropdown(\n",
    "    options=all_stop_n_id,\n",
    "    value=\"8591122\"\n",
    ")\n",
    "\n",
    "end=widgets.Dropdown(\n",
    "    options=all_stop_n_id,\n",
    "    value=\"8503500\"\n",
    ")\n",
    "\n",
    "date_picker=widgets.DatePicker(\n",
    "    description='Pick a Date',\n",
    "    value = datetime.date(2021,6,4)\n",
    ")\n",
    "\n",
    "hour=widgets.Dropdown(\n",
    "    options=range(7,21),\n",
    "    value=13,\n",
    "    description='Hour:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "minute=widgets.Dropdown(\n",
    "    options=range(0,60),\n",
    "    value=0,\n",
    "    description='Minute:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    #reset display\n",
    "    output.clear_output()\n",
    "\n",
    "    #get all value of widget\n",
    "\n",
    "    Q=slider.value\n",
    "    starting_stop=start.value\n",
    "    arrival_stop=end.value\n",
    "    \n",
    "    weekday=date_picker.value.weekday()\n",
    "    avoid =[]\n",
    "    \n",
    "    arrival_time=pd.to_datetime(\"%02d:%02d:00\"%(hour.value,minute.value))\n",
    "    \n",
    "    \n",
    "    with output:\n",
    "        #this line also print confidence level\n",
    "        paths = query(starting_stop, arrival_stop, arrival_time, weekday, Q, avoid)\n",
    "        n_paths = len(paths)\n",
    "        if n_paths==1: print('An optimal paths is found!')\n",
    "        else: print('{} optimal paths are found!'.format(n_paths))\n",
    "        \n",
    "        for i,path in enumerate(paths):\n",
    "            print('\\n\\nAlternative: ', i+1)\n",
    "            describe(path)\n",
    "            visualize_path(path)\n",
    "        \n",
    "\n",
    "#initalize button\n",
    "button = widgets.Button(\n",
    "    description='RUN',\n",
    "    button_style='info',\n",
    "    tooltip='RUN the research'\n",
    ")\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "#group all widget\n",
    "widget_by_row =[widgets.HBox([widgets.Label(\"Min Confidence Level\"), slider]),\n",
    "                widgets.HBox([widgets.Label(\"Starting station\"), start]),\n",
    "                widgets.HBox([widgets.Label(\"Ending station\"), end]),\n",
    "                date_picker,\n",
    "                widgets.HBox([hour,minute]),\n",
    "                button]\n",
    "#arrange them in a column\n",
    "all_widget_in_one=widgets.VBox(widget_by_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the cell below to launch the interactive journey search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "display(all_widget_in_one,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Validation of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example we could have one during rush hours, one during calm day-hour / one short journey, one long journey (if possible using different means of transports) / one allowing very low Q, one Q=99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This first cell is the reference. We will change some variables and see repercution on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "Q_0=0.95\n",
    "starting_stop_0=\"8591122\"\n",
    "arrival_stop_0=\"8503500\"\n",
    "weekday_0=4#Friday\n",
    "hour_0=13\n",
    "minute_0=0\n",
    "\n",
    "arrival_time_0=pd.to_datetime(\"%02d:%02d:00\"%(hour_0,minute_0))\n",
    "\n",
    "\n",
    "paths = query(starting_stop_0, arrival_stop_0, arrival_time_0, weekday_0, Q_0, [])\n",
    "n_paths = len(paths)\n",
    "if n_paths==1: print('An optimal paths is found!')\n",
    "else: print('{} optimal paths are found!'.format(n_paths))\n",
    "\n",
    "for i,path in enumerate(paths):\n",
    "    print('\\n\\nAlternative: ', i+1)\n",
    "    describe(path)\n",
    "    visualize_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell we will see that the day of the week have an influence on the result because there are in general less public transport connection on the weekend. The reference trip was on a Friday whereas this trip is on a Saturday. The two following suggested trip start earlier than the reference and need two changes instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "Q_0=0.5\n",
    "starting_stop_0=\"8591122\"\n",
    "arrival_stop_0=\"8503500\"\n",
    "weekday_0=5\n",
    "hour_0=13\n",
    "minute_0=0\n",
    "\n",
    "arrival_time_0=pd.to_datetime(\"%02d:%02d:00\"%(hour_0,minute_0))\n",
    "\n",
    "\n",
    "paths = query(starting_stop_0, arrival_stop_0, arrival_time_0, weekday_0, Q_0, [])\n",
    "n_paths = len(paths)\n",
    "if n_paths==1: print('An optimal paths is found!')\n",
    "else: print('{} optimal paths are found!'.format(n_paths))\n",
    "\n",
    "for i,path in enumerate(paths):\n",
    "    print('\\n\\nAlternative: ', i+1)\n",
    "    describe(path)\n",
    "    visualize_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we can see that the probability as an influence on the trip, if we allow less probable trip we can take a connection which start even later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "Q_0=0.75\n",
    "starting_stop_0=\"8591122\"\n",
    "arrival_stop_0=\"8503500\"\n",
    "weekday_0=4#Friday\n",
    "hour_0=13\n",
    "minute_0=0\n",
    "\n",
    "arrival_time_0=pd.to_datetime(\"%02d:%02d:00\"%(hour_0,minute_0))\n",
    "\n",
    "paths = query(starting_stop_0, arrival_stop_0, arrival_time_0, weekday_0, Q_0, [])\n",
    "n_paths = len(paths)\n",
    "if n_paths==1: print('An optimal paths is found!')\n",
    "else: print('{} optimal paths are found!'.format(n_paths))\n",
    "\n",
    "for i,path in enumerate(paths):\n",
    "    print('\\n\\nAlternative: ', i+1)\n",
    "    describe(path)\n",
    "    visualize_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's consider another longer path and compare it with GoogleMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "Q_0=0.95\n",
    "starting_stop_0=\"8590651\"\n",
    "arrival_stop_0=\"8503006:0:4\"\n",
    "weekday_0=4#Friday\n",
    "hour_0=13\n",
    "minute_0=0\n",
    "\n",
    "arrival_time_0=pd.to_datetime(\"%02d:%02d:00\"%(hour_0,minute_0))\n",
    "\n",
    "paths = query(starting_stop_0, arrival_stop_0, arrival_time_0, weekday_0, Q_0, [])\n",
    "\n",
    "n_paths = len(paths)\n",
    "if n_paths==1: print('An optimal paths is found!')\n",
    "else: print('{} optimal paths are found!'.format(n_paths))\n",
    "\n",
    "for i,path in enumerate(paths):\n",
    "    print('\\n\\nAlternative: ', i+1)\n",
    "    describe(path)\n",
    "    visualize_path(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This cell usually don't display automatically, just make any modification, like adding a space, and rerun the cell)\n",
    "\n",
    "![trip_image](../figs/trip_image.PNG)\n",
    "\n",
    "![trip_textual](../figs/trip_textual.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our second trip is equivalent to the trip suggested by GoogleMap because GoogleMap try to minimize the number of changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
